{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c768d1b-a100-47ad-8c2e-a66fde3b8049",
   "metadata": {},
   "source": [
    "# Lab 7: Neural Networks with Scikit-learn (Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bbffb7-d620-47f9-8f3e-60da49be0f05",
   "metadata": {},
   "source": [
    "#### Objective:\n",
    "In this lab, you will use Scikit-learn's Multi-layer Perceptron (MLP) to build a neural network model that predicts house prices using the **California Housing dataset**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab74ba7e-d701-464f-b171-3c5d39c37c6c",
   "metadata": {},
   "source": [
    "### Dataset Description:\n",
    "The California Housing dataset contains information about house prices from various regions in California. It is a regression task where we predict the median house value (target) based on features such as:\n",
    "\n",
    "    MedInc: Median income in the block.\n",
    "    HouseAge: Average age of houses in the block.\n",
    "    AveRooms: Average number of rooms per household.\n",
    "    AveOccup: Average number of household members.\n",
    "    Latitude and Longitude: Geographical location of the block.\n",
    "\n",
    "This dataset is commonly used for regression tasks and provides continuous target variables (house values). It offers a variety of features, such as socioeconomic factors and geographic location, that may influence house prices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ce37cc-9ce6-4c46-9eda-dcd3dad92576",
   "metadata": {},
   "source": [
    "### Load and explore the Dataset\n",
    "Start by importing the necessary libraries and loading the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "944cf465-37bc-4bb3-b6a7-35649e9eec02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load the dataset\n",
    "data = fetch_california_housing()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=41)\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339f48e8-13ae-485a-97b7-1f00da2bec33",
   "metadata": {},
   "source": [
    "### What is happening here?\n",
    "We're loading the dataset, splitting it into training and testing sets (80-20 split), and scaling the features to have zero mean and unit variance. This helps the neural network to learn faster and more efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2ed5fd-14eb-487c-87ef-b7b1e81baeaa",
   "metadata": {},
   "source": [
    "##### Build a Feed-forward Neural Network (MLP) using Scikit-learn:\n",
    "Scikit-learn provides an easy way to define a neural network using MLPRegressor. This model will use a simple multi-layer perceptron (feed-forward network). \n",
    "\n",
    "** Please note this task takes some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90d32b7c-d37f-4a62-a324-e18745f3c057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Test set R^2: 0.7078\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "mlp = MLPRegressor(max_iter=50)\n",
    "\n",
    "# Hyperparameter grid for tuning\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(5,), (10,), (15,)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['adam', 'sgd'],\n",
    "    'alpha': [0.0001, 0.001]\n",
    "}\n",
    "\n",
    "# Apply GridSearchCV for hyperparameter tuning\n",
    "grid_search = GridSearchCV(mlp, param_grid, cv=5, scoring='neg_mean_squared_error', verbose=1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best model after grid search\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_score = best_model.score(X_test, y_test)\n",
    "print(f\"Test set R^2: {test_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f4716c-4f34-402f-b75a-d018defaf6f9",
   "metadata": {},
   "source": [
    "##### Steps:\n",
    "- `MLPRegressor`: Creates a multi-layer perceptron regression model.\n",
    "- Hyperparameter Grid: We define the search space for hyperparameters such as:\n",
    " \n",
    "    `hidden_layer_sizes`: Number of neurons in the hidden layers.\n",
    "\n",
    "    `activation`: Activation function for hidden layers (tanh, relu).\n",
    "\n",
    "    `solver`: The solver used for weight optimization (Adam, SGD).\n",
    "\n",
    "    `alpha`: Regularization term (L2 penalty).\n",
    "\n",
    "- `GridSearchCV`: Performs grid search to find the best combination of hyperparameters using 5-fold cross-validation.\n",
    "- Model Evaluation: We evaluate the model's performance using R² on the test set.\n",
    "\n",
    "** R² (coefficient of determination): A measure of how well the model explains the variability of the target variable. The closer it is to 1, the better the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1e1d87-aed6-4b99-acaa-cb0bbde63710",
   "metadata": {},
   "source": [
    "### Implement Regularisation and Optimisation:\n",
    "Now, let's apply **L2** Regularisation (ridge) and explore different solvers like **Adam** and **SGD**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "000b5797-62a7-4740-8ac8-284d9011b23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Test set R^2 with Regularisation: 0.6912\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(5,), (10,), (15,)],\n",
    "    'activation': ['relu'],\n",
    "    'solver': ['adam', 'sgd'],\n",
    "    'alpha': [0.0001, 0.001],  # L2 regularisation strength\n",
    "    'learning_rate': ['constant', 'adaptive']\n",
    "}\n",
    "\n",
    "#mlp= MLPRegressor(max_iter=50)\n",
    "grid_search = GridSearchCV(mlp, param_grid, cv=5, scoring='neg_mean_squared_error', verbose=1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "test_score = best_model.score(X_test, y_test)\n",
    "print(f\"Test set R^2 with Regularisation: {test_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f9c76b-7ab5-40fd-a312-322df5038c4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
